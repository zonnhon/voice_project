import os
import time
import queue
import json
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, List

import numpy as np
import sounddevice as sd
import soundfile as sf
import webrtcvad
import requests

# ===== GPT API è¨­å®š =====
GPT_API_BASE_URL = os.getenv("GPT_API_BASE_URL", "").rstrip("/")
GPT_API_KEY = os.getenv("GPT_API_KEY", "")
GPT_MODEL_ID = os.getenv("GPT_MODEL_ID", "gpt-4o-mini")

SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€å€‹èªžéŸ³åŠ©ç†ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›žç­”ä½¿ç”¨è€…çš„å•é¡Œï¼Œ"
    "å›žç­”è¦ç°¡æ½”ã€å£èªžã€é©åˆç›´æŽ¥å”¸å‡ºä¾†ã€‚"
)


# ========= è¨­å®šå€ =========
SAMPLE_RATE = 16000          # WebRTC VAD å»ºè­° 8k/16k/32k/48k
CHANNELS = 1
FRAME_MS = 20                # WebRTC VAD æ”¯æ´ 10/20/30ms
VAD_MODE = 2                 # 0~3 è¶Šå¤§è¶Šåš´æ ¼ï¼ˆèª¤è§¸ç™¼æ›´å°‘ï¼Œä½†å¯èƒ½æ¼ï¼‰
PRE_ROLL_MS = 300            # é–‹å£å‰ç·©è¡ï¼ˆé¿å…åˆ‡åˆ°ç¬¬ä¸€å€‹å­—ï¼‰
SILENCE_END_MS = 900         # éœéŸ³å¤šä¹…è¦–ç‚ºä¸€å¥è©±çµæŸ
MIN_UTTERANCE_MS = 400       # å¤ªçŸ­çš„ç‰‡æ®µä¸é€ï¼ˆé¿å…å™ªéŸ³èª¤è§¸ç™¼ï¼‰
MAX_UTTERANCE_MS = 15000     # æœ€é•·ä¸€å¥è©±ï¼ˆé¿å…ç„¡é™éŒ„ï¼‰
OUTPUT_DIR = "recordings"    # å­˜ wav çš„è³‡æ–™å¤¾
DEVICE_INDEX: Optional[int] = None  # None=é è¨­éº¥å…‹é¢¨ï¼›æˆ–å¡«æ•´æ•¸ index


# Whisper APIï¼ˆç”¨ç’°å¢ƒè®Šæ•¸ï¼Œä¹Ÿå¯ç›´æŽ¥æ”¹å¸¸æ•¸ï¼‰
SPEECHES_BASE_URL = os.getenv("SPEACHES_BASE_URL", "").rstrip("/")
TRANSCRIPTION_MODEL_ID = os.getenv("TRANSCRIPTION_MODEL_ID", "whisper-small")

API_TIMEOUT_SEC = 60

def ensure_gpt_env():
    if not GPT_API_BASE_URL:
        raise RuntimeError("è«‹è¨­å®š GPT_API_BASE_URL")
    if not GPT_MODEL_ID:
        raise RuntimeError("è«‹è¨­å®š GPT_MODEL_ID")

# ========= ç¨‹å¼ä¸»é«” =========
@dataclass
class Segment:
    wav_path: str
    started_at: float
    ended_at: float


def ensure_env():
    if not SPEECHES_BASE_URL:
        raise RuntimeError(
            "è«‹è¨­å®šç’°å¢ƒè®Šæ•¸ SPEECHES_BASE_URLï¼Œä¾‹å¦‚ï¼š\n"
            "  export SPEECHES_BASE_URL='http://127.0.0.1:8000'\n"
            "æˆ–åœ¨ç¨‹å¼ä¸­ç›´æŽ¥æŒ‡å®š SPEECHES_BASE_URL å¸¸æ•¸ã€‚"
        )


def pcm16_bytes_from_float32(x: np.ndarray) -> bytes:
    """
    sounddevice callback çµ¦çš„é€šå¸¸æ˜¯ float32 [-1,1]ï¼Œè½‰æˆ PCM16 bytes çµ¦ webrtcvadã€‚
    x: shape (n, 1) æˆ– (n,)
    """
    if x.ndim == 2:
        x = x[:, 0]
    x = np.clip(x, -1.0, 1.0)
    pcm16 = (x * 32767.0).astype(np.int16)
    return pcm16.tobytes()


def save_wav(path: str, audio_float32: np.ndarray, samplerate: int = SAMPLE_RATE):
    # audio_float32 shape (n,1) for soundfile
    sf.write(path, audio_float32, samplerate, subtype="PCM_16")


def call_whisper_api(wav_path: str) -> dict:
    """
    ç­‰æ•ˆæ–¼ï¼š
    curl -s "$SPEACHES_BASE_URL/v1/audio/transcriptions" \
      -F "file=@audio.wav" -F "model=$TRANSCRIPTION_MODEL_ID"
    """
    url = f"{SPEECHES_BASE_URL}/v1/audio/transcriptions"
    with open(wav_path, "rb") as f:
        files = {"file": (os.path.basename(wav_path), f, "audio/wav")}
        data = {"model": TRANSCRIPTION_MODEL_ID}
        r = requests.post(url, files=files, data=data, timeout=API_TIMEOUT_SEC)
    r.raise_for_status()
    # å¸¸è¦‹å›žå‚³ï¼š{"text":"..."} æˆ–æ›´å®Œæ•´ json
    try:
        return r.json()
    except Exception:
        return {"raw": r.text}

def call_gpt_api(user_text: str) -> str:
    """
    OpenAI Chat Completions ç›¸å®¹ API
    """
    url = f"{GPT_API_BASE_URL}/chat/completions"

    headers = {
        "Content-Type": "application/json",
    }
    if GPT_API_KEY:
        headers["Authorization"] = f"Bearer {GPT_API_KEY}"

    payload = {
        "model": GPT_MODEL_ID,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_text},
        ],
        "temperature": 0.4,
    }

    r = requests.post(url, headers=headers, json=payload, timeout=60)
    r.raise_for_status()

    data = r.json()
    return data["choices"][0]["message"]["content"]


def main():
    ensure_env()
    ensure_gpt_env()
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    vad = webrtcvad.Vad(VAD_MODE)

    frame_samples = int(SAMPLE_RATE * FRAME_MS / 1000)  # 20ms @16k => 320 samples
    silence_frames_end = int(SILENCE_END_MS / FRAME_MS)
    pre_roll_frames = int(PRE_ROLL_MS / FRAME_MS)
    min_frames = int(MIN_UTTERANCE_MS / FRAME_MS)
    max_frames = int(MAX_UTTERANCE_MS / FRAME_MS)

    q: "queue.Queue[np.ndarray]" = queue.Queue()

    def callback(indata, frames, time_info, status):
        if status:
            # ä½ ä¹Ÿå¯ä»¥æ”¹æˆ print(status) è§€å¯Ÿ underrun/overflow
            pass
        # indata shape (frames, channels)
        q.put(indata.copy())

    print(
        "=== VAD -> record -> Whisper API transcribe ===\n"
        f"SPEECHES_BASE_URL: {SPEECHES_BASE_URL}\n"
        f"MODEL: {TRANSCRIPTION_MODEL_ID}\n"
        f"Device: {DEVICE_INDEX if DEVICE_INDEX is not None else 'default'}\n"
        "Speak to trigger. Ctrl+C to stop.\n"
    )

    # pre-roll ring buffer (float32 frames)
    pre_roll: Deque[np.ndarray] = deque(maxlen=pre_roll_frames)

    recording = False
    voiced_frames: List[np.ndarray] = []
    silence_run = 0
    utter_frames = 0
    seg_count = 0

    with sd.InputStream(
        samplerate=SAMPLE_RATE,
        channels=CHANNELS,
        blocksize=frame_samples,  # è®“ callback æ¯æ¬¡å‰›å¥½ä¸€å€‹ frame
        dtype="float32",
        device=DEVICE_INDEX,
        callback=callback,
    ):
        while True:
            frame = q.get()  # shape (frame_samples, 1)
            pre_roll.append(frame)

            # VAD åˆ¤æ–·éœ€è¦ PCM16 bytes ä¸” frame size è¦ç¬¦åˆ 10/20/30ms
            pcm16 = pcm16_bytes_from_float32(frame)
            is_speech = vad.is_speech(pcm16, SAMPLE_RATE)

            if not recording:
                if is_speech:
                    recording = True
                    silence_run = 0
                    utter_frames = 0
                    voiced_frames = list(pre_roll)  # å…ˆæŠŠ pre-roll å¡žé€²åŽ»
                    print("ðŸŽ¤ ON")
                continue

            # recording == True
            voiced_frames.append(frame)
            utter_frames += 1

            if is_speech:
                silence_run = 0
            else:
                silence_run += 1

            # å¥å­çµæŸæ¢ä»¶ï¼šéœéŸ³ä¸€æ®µæ™‚é–“ æˆ– å¤ªé•·
            end_by_silence = silence_run >= silence_frames_end
            end_by_maxlen = utter_frames >= max_frames

            if end_by_silence or end_by_maxlen:
                recording = False
                print("ðŸŽ¤ OFF" + (" (maxlen)" if end_by_maxlen else ""))

                if utter_frames < min_frames:
                    # å¤ªçŸ­ï¼Œä¸ŸæŽ‰ï¼ˆé€šå¸¸æ˜¯èª¤è§¸ç™¼ï¼‰
                    print("  (skip: too short)")
                    voiced_frames.clear()
                    pre_roll.clear()
                    continue

                # åˆä½µä¸¦å­˜æª”
                audio = np.concatenate(voiced_frames, axis=0)  # (N,1)
                ts = time.strftime("%Y%m%d_%H%M%S")
                seg_count += 1
                wav_path = os.path.join(OUTPUT_DIR, f"utt_{ts}_{seg_count:03d}.wav")
                save_wav(wav_path, audio, SAMPLE_RATE)
                print(f"  saved: {wav_path}")

                # å‘¼å« Whisper API
                try:
                    resp = call_whisper_api(wav_path)
                    # å¸¸è¦‹ key: text
                    # text = resp.get("text")
                    # if text is None:
                    #     # fallback: å°å‡º json
                    #     print("  transcription:", json.dumps(resp, ensure_ascii=False))
                    # else:
                    #     print("  transcription:", text)

                    text = resp.get("text")
                    if not text:
                        print("  transcription:", json.dumps(resp, ensure_ascii=False))
                    else:
                        print("  transcription:", text)

                        # ===== å‘¼å« GPT =====
                        try:
                            reply = call_gpt_api(text)
                            print("ðŸ¤– GPT reply:", reply)
                        except requests.RequestException as e:
                            print("  GPT API error:", str(e))
                except requests.RequestException as e:
                    print("  API error:", str(e))

                # é‡ç½®
                voiced_frames.clear()
                pre_roll.clear()


if __name__ == "__main__":
    main()
